
# This file documents all available configuration options for domain adaptation
# experiments. Copy this file and modify values as needed for your experiments.

# --------------------
# Experiment Settings
# --------------------

experiment_name: "base.template" # the name of the experiment
seed: 42 # random seed for reproducibility 

output:
  # Base directory for all experiment outputs
  root_dir: "experiments"
  # setting this will create subdirectories for checkpoints and logs
  #   - experiments/experiment_name/logs/          (training metrics CSV)
  #   - experiments/experiment_name/checkpoints/   (model weights)
  #   - experiments/experiment_name/figures/       (plots and visualizations)
  log_level: 10 # 10 = DEBUG, 20 = INFO, 30 = WARNING, 40 = ERROR, 50 = CRITICAL
  # logs.log will be saved under root_dir

# --------------------
# Data Configuration
# --------------------

data:
  # --- Source Domain ---
  # path to source domain images
  source_img_dir: "data/source/galaxy_images_rgb"
  # path to source domain labels, can be a CSV or JSON file
  source_labels: "data/source/source_galaxy_labels.csv"

  include_rotations: false # data augmentation for source domain
  # NOTE:
  #   - If model.type == escnn (equivariant): prefer include_rotations: false
  #       The encoder is rotation/flip equivariant (C_N / D_N), so duplicating
  #       rotated copies adds little and can bias orientation statistics.
  #   - If model.type in {cnn, resnet}: you can consider include_rotations: true
  #       Standard backbones may benefit from rotation aug under covariate shift.
  # NOTE: include_rotations=True will include 8 augmented images for each original image.
  # The augmented images are created by rotating the original image by 0, 90, 180, and 270 degrees 
  # and flipping the image horizontally. It will result in 25856 total images for the source domain.
  
  # Disabling it will result in 3232 total original images for the source domain.

  # --- Target Domain ---
  # path to target domain images
  target_img_dir: "data/target/gz2_images"  # for GZ2 dataset
  # target_img_dir: "data/target/gz_desi"   # for DESI dataset
  # path to target domain labels, can be a CSV or JSON file
  target_labels: "data/target/gz2_galaxy_labels_master.csv" # for GZ2 full dataset
  # target_labels: "data/target/gz2_galaxy_labels.csv"      # for GZ2 top N dataset
  # target_labels: "data/target/gz_desi_labels.csv"         # for DESI dataset

  # Optional: Provide precomputed normalization statistics
  # The below examples were calculated using the nebula/data/normalization.py script.
  # Choose the one that matches your dataset.
  # If not provided, will be computed automatically
  #source_mean: [0.10848670452833176, 0.117486372590065, 0.09117863327264786]
  #source_std:  [0.2009781301021576, 0.21104967594146729, 0.1836480051279068]
  
  # target_mean: [0.04405268281698227, 0.03992829844355583, 0.032875481992959976]    # GZ2 mean from data/target/gz2_galaxy_labels_master.csv
  # target_std: [0.0869700163602829, 0.0758928582072258, 0.07477222383022308]     # GZ2 std from data/target/gz2_galaxy_labels_master.csv
  
  # target_mean: [0.042620349675416946, 0.03869814798235893, 0.03188521787524223]   # Full GZ2 mean from data/target/gz2_galaxy_labels_master.csv
  # target_std: [0.08519686758518219, 0.0742286816239357, 0.07386281341314316]      # Full GZ2 std from data/target/gz2_galaxy_labels_master.csv
  
  # target_mean: [0.1059635579586029, 0.09898710995912552, 0.09673696756362915]    # DESI mean from data/target/gz_desi_labels.csv
  # target_std: [0.09570237994194031, 0.09059888124465942, 0.08683059364557266]    # DESI std from data/target/gz_desi_labels.csv
  
  # source_mean: [0.485, 0.456, 0.406] # ImageNet mean
  # source_std: [0.229, 0.224, 0.225] # ImageNet std
  # target_mean: [0.485, 0.456, 0.406] # ImageNet mean
  # target_std: [0.229, 0.224, 0.225] # ImageNet std

  # Normalization strategy
  shared_norm: false
  # Use same normalization statistics for source and target
  # true: Normalize both domains using source statistics:
  #       target_mean = source_mean, target_std = source_std

  # false: Use separate statistics

  # Resize all images to this size [height, width]
  image_size: [120, 120]
  

  # Number of samples per batch
  batch_size: 128
  
  # Fraction of data to use for validation,
  # NOTE: we perform a stratified split to maintain class proportions.
  # The idea is to split the dataset into train and val sets while maintaining the class proportions
  # Example:
  # Suppose our 3 classes have the following counts: [10, 20, 70]
  # With val_size = 0.2, the validation set will get 20% from each class:
  #   Class 0: 2 val, 8 train
  #   Class 1: 4 val, 16 train
  #   Class 2: 14 val, 56 train
  val_size: 0.2

  # Number of parallel data loading workers
  # 0 = load in main process, >0 = parallel loading
  num_workers: 4

  # Use WeightedRandomSampler for source training data
  # WARNING: DO NOT use sampler with use_class_weights=True (double weighting!)
  # WeightedRandomSampler oversamples underrepresented classes
  # Recommended: use_sampler=false + use_class_weights=true (focal loss handles it)
  use_sampler: false
  
  # Sampler parameters (only used if use_sampler=true)
  # Compute weights based on smoothing and power settings:
  #   - smoothing=0, power=1.0: standard inverse frequency (1.0 / count)
  #   - smoothing=0, power!=1.0: power scaling (1.0 / count ** power)
  #   - smoothing>0: smoothed weights (1.0 / (count + smoothing) ** power)
  sampler_smoothing: 0.0
  sampler_power: 1.0
  sampler_replacement: true


# Model architecture
# Supported: 'cnn' | 'resnet' | 'escnn'
model:
  # Select model type
  type: "cnn"   # cnn | resnet | escnn

  # --- ResNet options (only used when type: resnet) ---
  # torchvision resnet family: resnet18 | resnet34 | resnet50 | ...
  # type: resnet
  # arch: "resnet18"
  # pretrained: true          # use ImageNet weights
  # trainable_layers: 2       # unfreeze: 1→layer4; 2→layer4+layer3
  # dropout: 0.3              # MLP head dropout

  # --- ESCNN options (only used when type: escnn) ---
  # Symmetry group: C_N (rotations) or D_N (rotations+flips)
  # NOTE: Its better to set include_rotations=False
  # type: escnn
  # group: "C8"               # "C4", "C8", "D4", "D8", ...
  # N: 8                      # group order for rotations
  # base_width: 16            # stage-1 width (stages use 1x/2x/4x)
  # dropout: 0.3              # MLP head dropout


training:
  # ------------------------------------------------ BASE CONFIG ------------------------------------------------

  # This base configs are mainly general, so they are common for all methods.
  num_epochs: 50            # Total number of training epochs, any positive integer value
  warmup_epochs: 0           # Number of epochs to warmup the model before starting the DA loss, any positive integer value
  lr: 1e-3                  # Learning rate, any positive float value
  optimizer: "adamw"        # 'adam', 'adamw', 'sgd'
  weight_decay: 1e-2        # Weight decay for regularization
  max_norm: 10.0            # Gradient clipping threshold, used in the backward pass to prevent exploding gradients (0 = no clipping)
  
  # Learning rate scheduling
  lr_scheduler: null        # Options: null | "cosine" | "step" | "exponential"
  # null: no scheduling (constant lr)
  # "cosine": cosine annealing from lr to min_lr over num_epochs
  # "step": reduce lr by 0.1 every num_epochs//3 epochs
  # "exponential": lr *= 0.95 every epoch
  min_lr: 1e-6              # Minimum learning rate for cosine annealing (only used if lr_scheduler="cosine")
  
  criterion: "cross_entropy"  # Loss function: 'cross_entropy', 'focal'

  # Class weighting
  # WARNING: DO NOT use use_class_weights=True with use_sampler=True (double weighting!)
  # Recommended: use_class_weights=true + use_sampler=false
  use_class_weights: false          # True to apply class weights, weights will be computed based on the class distribution in the source domain and applied to the loss function (criterion)
  class_weight_method: "effective"  # 'effective' or 'balanced' (only if use_class_weights=True)
  class_weight_beta: 0.9999         # Only used with 'effective' method (only if class_weight_method="effective")

  # Focal loss parameters (only used if criterion='focal')
  # ---------------------------------------------------------------------------------------
  # focal_gamma:
  #   This is the focusing parameter γ (gamma), it controls how much to focus on hard examples
  #   - Higher Y >= 2.0: increases the focus on misclassified examples.
  #   - Lower Y = ~1.0: behaves closer to CrossEntropy.
  #   - Y = 0.0: equivalent to standard CrossEntropy
  focal_gamma: 2.0

  # focal_alpha:
  #   α (alpha) class weighting in focal loss, 
  #    it controls class-level balancing 
  #    α_t in the original focal loss paper 
  #    by Tsung-Yi Lin et al., 2018 https://arxiv.org/pdf/1708.02002).
  #
  #   Possible values:
  #     null  → no alpha weighting (treat all classes equally)
  #     float → same scalar applied to all classes (e.g., 0.25 for rare class, 0.75 for others)
  #     [list] → custom per-class alpha weights (must be a length of 3, aka the number of classes)
  #     "class_weights" → automatically use computed class_weights as alpha_t
  #
  #   IMPORTANT:
  #     - If focal_alpha == "class_weights" and use_class_weights=True,
  #         then the same computed class weights are passed to α_t (recommended for imbalanced data)
  #     - If you instead want to apply weights like in standard CrossEntropyLoss,
  #         set focal_alpha = null and use_class_weights=True (weights will be passed)
  #
  #   Summary:
  #     focal_alpha = "class_weights" → use α_t balancing (original focal loss behavior)
  #     focal_alpha = null + use_class_weights=True → use CE-style weighting (rarely used in focal loss)
  #
  focal_alpha: null   # Options: null | float | [per-class weights] | "class_weights"
  
  # examples:
  # focal_alpha: "class_weights"
  # focal_alpha: [0.25, 0.75, 0.75] # per-class alpha weights, classes 
  # focal_alpha: 0.25

  # NOTE:
  #   class_weights are computed automatically based on class imbalance when use_class_weights=True.
  #   Setting focal_alpha="class_weights" ensures they are applied to α_t, not as CE weights.

  # Reduction method for focal loss ('mean' or 'sum')
  focal_reduction: "mean"

  # Early stopping configuration
  early_stopping_patience: 5       # Options: None | integer (stop if validation metric doesn't improve after these many epochs)
  # NOTE: (Metric to monitor for early stopping)
  
  early_stopping_metric: "f1"  # Options: "train_loss"| "ce_loss" | "da_loss" | "f1" | "accuracy" 
  # NOTE: "f1" and "accuracy" require target_loader to be provided (doesn't work in baseline where we only pass source loader)


  # ------------------------------------------------ DA METHODS CONFIG ------------------------------------------------
  
  # Choose the domain adaptation method to use, the rest of the parameters are specific to the method
  method: "baseline"  # Options: 'baseline', 'adversarial', 'sinkhorn', 'mmd', 'energy'

  # ========================================
  # Optional losses (available for all DA methods)
  # ========================================
  
  # Entropy minimization on target predictions
  # Available for: all DA methods (adversarial, sinkhorn, mmd, energy)
  lambda_entropy: 0.0                # Weight for entropy loss on target predictions
  #  WARNING: lambda_entropy > 0.1 can cause model collapse to majority class!
  # 
  # Entropy minimization encourages confident predictions, but without target supervision,
  # the model can become too confident in wrong predictions (all -> spiral class).
  # RECOMMENDED: lambda_entropy in [0.01, 0.1] range, or set to 0.0 to disable
  # Monitor class diversity during training if using entropy minimization
  
  # Optional OT-based alignment loss (from modeling.ot_loss.OTAlignmentLoss)
  # Available for: all DA methods (adversarial, sinkhorn, mmd, energy)
  # This is a purely feature-based loss that operates on source/target latents (no labels).
  # It combines Sinkhorn OT distance, matched-pair MSE, and top-k worst pair costs.
  lambda_ot: 0.0                     # Weight for OT-based alignment loss (0.0 = disabled)
  # This loss is added on top of the standard DA loss:
  # Total Loss = L_CE + L_domain + lambda_entropy * L_entropy + lambda_ot * L_OT
  
  # OT loss hyperparameters (only used if lambda_ot > 0.0)
  ot_lambda_ot: 0.3                  # Weight for Sinkhorn OT distance component
  ot_lambda_match: 0.6               # Weight for matched-pair MSE component
  ot_lambda_topk: 0.1                # Weight for top-k worst pair cost component
  ot_epsilon: 0.05                   # Regularization parameter for Sinkhorn OT
  ot_n_iter: 50                      # Number of iterations for Sinkhorn OT
  match_epsilon: 0.05                # Regularization parameter for matching
  match_n_iter: 20                   # Number of iterations for matching
  ot_topk: 5                         # Number of worst pairs to consider for top-k loss

  # ========================================
  # Adversarial DA parameters
  # (ONLY used if method='adversarial')
  # ========================================
  
  lambda_grl: 0.25                   # Gradient reversal scaling (constant value if no schedule)
  
  # Lambda GRL scheduling (NEW) - gradually ramp up adversarial strength
  # Recommended for stability: start from 0.0 and ramp up to final value
  lambda_grl_schedule: null          # Options: null (use constant lambda_grl) | dict with schedule config
  # Example:
  # lambda_grl_schedule:
  #   start: 0.0                     # Starting lambda value
  #   end: 0.4                       # Final lambda value
  #   type: "sigmoid"                # Schedule type: "linear" | "cosine" | "sigmoid" | "step" | "polynomial" | "constant"
  #   # Optional parameters for sigmoid:
  #   # midpoint: 0.5                # Fraction of training where lambda reaches midpoint (0.0-1.0)
  #   # scale: 0.1                   # Controls steepness of sigmoid curve
  #   # Optional parameters for step:
  #   # step_size: 10                # Increase lambda every N epochs
  #   # Optional parameters for polynomial:
  #   # power: 2.0                   # Polynomial power
  
  # latent_dim: null                 # Dimension of latent space from feature extractor, automatically calculated from the model if not provided
  domain_hidden_dim: 256             # Hidden units in domain classifier
  use_projection: true               # Whether to use a projection layer before domain classifier
  domain_projection_dim: 128         # Dimension of domain projection layer (only if use_projection=True)

  # ========================================
  # OT-based DA parameters (Sinkhorn/MMD/Energy)
  # (ONLY used if method in ["sinkhorn", "mmd", "energy"])
  # ========================================
  
  # Fixed weight for the domain adaptation loss
  # Total Loss = L_CE + (lambda_da * L_DA) + lambda_entropy * L_entropy + lambda_ot * L_OT
  lambda_da: 0.1                     # Domain adaptation loss weight
  sinkhorn_blur: 10.0                # Sinkhorn regularization parameter (blur/epsilon)
  sinkhorn_p: 2                      # Sinkhorn distance exponent (p-norm)

  # Trainable weights (learn relative importance of CE vs DA loss)
  use_trainable_weights: false       # True to learn importance weights for multi-task learning
  eta_1_init: 0.1                    # Initial eta_1 for classification loss (if use_trainable_weights=True)
  eta_2_init: 1.0                    # Initial eta_2 for DA loss (if use_trainable_weights=True)

  # Sigma scheduling (anneal blur parameter during training)
  use_sigma_schedule: false          # True to apply schedule to sigma/blur parameter
  
  sigma_schedule_type: "exponential" # Options: 'exponential' | 'linear' | 'cosine' | 'step' | 'polynomial' | 'constant'
  # Supported schedules:
        # - exponential: sigma = sigma_initial_blur * (sigma_decay_rate ** epoch)
        # - linear: sigma = sigma_initial_blur - (sigma_initial_blur - sigma_final_blur) * (epoch / num_epochs)
        # - cosine: sigma = sigma_final_blur + 0.5 * (sigma_initial_blur - sigma_final_blur) * (1 + cos(pi * epoch / num_epochs))
        # - step: sigma = sigma_initial_blur * (sigma_step_gamma ** (epoch // sigma_step_size))
        # - polynomial: sigma = (sigma_initial_blur - sigma_final_blur) * (1 - epoch/num_epochs)^sigma_poly_power + sigma_final_blur
        # - constant: sigma = sigma_initial_blur (no decay)
  
  sigma_initial_blur: 10.0           # Starting blur value
  sigma_decay_rate: 0.95             # Exponential decay rate (for long training use 0.9-0.95, not 0.6!)
  sigma_final_blur: 1.0              # Final blur value (for linear/cosine/polynomial)
  sigma_min_blur: 0.1                # Minimum blur floor to prevent numerical underflow
  
  # IMPORTANT: For long training (>50 epochs) with exponential decay:
  # - Use milder decay rate (0.9-0.95) OR rely on sigma_min_blur floor
  # - Tutorial used 6 epochs where 0.6^6 ≈ 0.047 is safe
  # - For 100 epochs: 0.6^100 → 0 (underflow!), but 0.95^100 ≈ 0.0059 (OK)
  
  sigma_step_size: 2                 # Step size for step schedule
  sigma_step_gamma: 0.5              # Decay factor for step schedule
  sigma_poly_power: 2.0              # Polynomial power for polynomial schedule
